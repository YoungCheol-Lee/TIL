# 앙상블
> 여러 개의 분류기(Classifier)를 생성하고 예측을 결합 = 정확한 최종 예측 도출


## 앙상블 유형
1. 보팅(Voting)
2. 배깅(Bagging) 
   - 랜덤 포레스트 알고리즘: 결정 트리 알고리즘
3. 부스팅(Boosting) 
   - 에이다 부스팅, 그라디언트 부스팅, XGBoost, LightGBM: 결정 트리 알고리즘
4. 스태킹(Stacking)

- 오히려 뛰어난 성능의 모델들만 결합하는 것 보다 서로 다른 유형의 모델을 섞는 것이 성능에 도움

### 1. 보팅(Voting)과 배깅(Bagging)
> 여러 개의 분류기가 `투표`로 최종 예측 결과를 결정
- 보팅과 배깅의 차이
- 보팅
    - 서로 다른 알고리즘을 가진 분류기 결합 & 데이터가 동일
  
- 배깅
    - 같은 알고리즘을 가진 분류기 결합 & 데이터가 샘플링이 랜덤

#### 1.1. 보팅
- 보팅 종류: Hard Voting, Soft Voting
1. Hard Voting
- 다수결로 결정
2. Soft Voting
- class 확률을 평균
  - 만약, 분류기(Classifier)가 3가지 & 선택지(Class) 2가지 일 때, 각각의 1번 분류기(0.7/0.3), 2번 분류기(0.2/0.8), 3번 분류기(0.8/0.2) 가정
  - 각 선택지(Class)의 확률의 평균으로 결정

#### 1.2.배깅
- Bootstrap Sampling의 줄임말

> Bootstrap이란?   
> 기존 학습 데이터 세트(전체 데이터)에서 랜덤하게 `복원(중복) 추출`하여 `동일한 N크기의 데이터 세트 X개`를 만들고, `동일한 알고리즘`으로 학습

##### 1.2.1. 배깅의 대표 알고리즘
- 랜덤 포레스트
    - `여러 개의 결정 트리 분류기`가 배깅으로 데이터 샘플링 & 학습 => 모든 분류기가 보팅으로 예측 결정

- 주요 하이퍼 파라미터
- **`n_estimators`**: 랜덤 포레스트에서 결정 트리의 개수 지정(기본 = 100개). **주로 사용**
  - 설정 개수와 성능은 비례, But 무조건 향상은 아님
  - 트리 개수가 많아지면 학습 수행 시간 증가
- max_features: 결정 트리에 사용된 max_featurers와 같음
  - auto로 두는 편
- max_depth, min_samples_leaf 등도 사용 가능

### 2. 부스팅
> 여러 개의 약산 학습기(과소적합)를 순차적으로 학습

> 예측한 데이터나 학습 트리에 **`가중치 부여`** 를 통해 **`오류를 개선`** 해 나가면서 학습

#### 2.1. 에이다 부스팅(Adaptive Boosting)
> 틀린 Data에 가중치 부여(현재는 거의 사용 X)   

1. step by step으로 이전 단계에서 잘못 예측된 데이터에 가중치를 부여(중요한 값으로 선정)
2. 다음 단계에서 가중치에 맞춰 분할 진행
   - 반복 작업하여 최종 학습/예측

#### 2.2. 그라디언트 부스팅
> 가중치 업데이터를 **`경사 하강법`** 을 이용

> 에러를 수정하기 위한 가중치 업데이터
![그라디언트 부스팅](%EC%9D%B4%EB%AF%B8%EC%A7%80%ED%8F%B4%EB%8F%84/GBM(%EA%B7%B8%EB%9D%BC%EB%94%94%EC%96%B8%ED%8A%B8%EB%B6%80%EC%8A%A4%ED%8C%85).png)

- **`경사하강법`**: 오류를 낮추기 위해 가중치를 갱신하는(가중치 업데이터) 방법


- 결정 트리의 개수 = 학습 횟수 = 경사하강법 수행 횟수

##### 2.2.1. GBM 하이퍼 파라미터
1. loss: 경사 하강법에서 사용할 손실 함수 지정(기본=deviance)
2. learning_rate: 