# 평가

## 1. 정확도
> 정확도(Accuracy) = 예측 결과가 동일한 데이터 수 / 전체 예측 데이터 수
**`정확도와 모델 성능은 비례관계 X`**

- Label 값이 불균형한 모델에서 정확도는 높게 나올 수 있음. 단, 정확도가 높다고 모델 성능이 좋은건 아님.

### 2. 오차 행렬(Confusion Matrix)
> 이진 분류의 예측 오류의 정도와 종류를 나타냄

| | 예측(Negative, 0) | 예측(Positive, 1) |
|:-:|:-:|:-:|
|실제(Negative, 0) | TN(True Negative) | FP(False Positive) |
|실제(Positive, 1) | FN(False Negative) | TP(True Positive) |

- 위 TN, FN, FP, TP 에서 Positive, Negative는 `모델의 예측`
    - TN: 모델이 Negative로 예측 = 실제도 Negative라서 True Negative(TN)
    - FN: 모델이 Negative로 예측 = 실제는 Postive라서 False Negative(FN)

#### 3. 정밀도(Precision)와 재현율(Recall)
1. 정밀도 = TP / (FP + TP) : Positive 예측 성공률
    - 정밀도가 더 중요한 지표 case = 스팸 메일
    - 실제 Negative인데 Positive로 예측한 경우 = FP를 낮춰야함 = 정밀도 증가
2. 재현율 = TP / (FN + TP) : 실제 Positive 비율(TPR, True Positive ratio라고도 부름)
    - 재현율이 더 중요한 지표 case = 암 진단, 금융 사기 판별 등
    - 실제 Positive인데 Negative로 예측한 경우 = FN을 낮춰야함 = 재현율 증가

##### 3.1. 정밀도/재현율 트레이드 오프
> 트레이드 오프: 정밀도와 재현율이 반비례 관계




#### F1 Score
> 정밀도와 재현율을 결합한 지표   
> 정밀도와 재현율이 어느 한쪽으로도 치우치지 않을 때 = F1 Score 최대값

$F1 = \frac{2}{\frac{1}{recall} + \frac{1}{precision}} = 2 * \frac{precision * recall}{precision + recall}$

