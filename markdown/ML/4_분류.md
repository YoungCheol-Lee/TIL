# 분류

1. 나이브 베이즈(Naive Bayes): 베이즈 통계와 생성 모델에 기반
2. 로지스틱 회귀(Logistic Regression): 독립변수와 종속변수 선형 관계성에 기반 
3. 결정 트리(Decision Tree): 데이터 균일도에 따른 규칙 기반 
4. 서포트 벡터 머신(Support Vector Machine): 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌
5. 최소 근접 알고리즘 (Nearest Neighbor): 근접 거리를 기준
6. 심층 연결 기반의 신경망
7. 앙상블(Ensemble): 서로 다른(또는 같은) 머신러닝 알고리즘을 결합

## 결정 트리
> 트리 기반 분류 규칙을 데이터에 있는 규칙을 학습하여 자동으로 만듦

- 정보 이득(Information Gain)
    - `분할 기준`으로 사용(정보 이득이 높은 속성 기준으로 분할)
    - `엔트로피` 개념을 기반으로 데이터의 혼잡도 사용
    - 데이터가 서로 다른 값이 섞여 있으면 엔트로피 증가
    - 정보 이득지수 = 1 - 엔트로피 지수

- 지니 계수
  - 불평등 지수를 나타내는 지수
  - 지니 계수가 낮은 속성 기준으로 분할
  - 0이 가장 평등, 1로 갈수록 불평등

### 결정 트리의 규칙 노드 생성 프로세스
1. 지니계수가 0인지 확인
    - 데이터 집합의 모든 아이템이 같은 분류에 속하는지 확인
2. 지니계수가 0이라면, 리프 노드로 만들어서 분류
    - 지니계수가 0이 아니라면, 데이터 분할에 적합한 기준(정보 이득 or 지니 계수) 찾기
3. 특정 기준으로 데이터 분할 & 규칙 노드 생성
- 모든 데이터 집합의 분류가 결정될 때까지 반복

#### 결정트리 장단점
- 장점
  - 쉽고 직관적임. 시각화로 모델의 학습을 보기 쉬움
  - Feature의 스케일링이나 정규화 등의 사전 가공 영향도가 작음
- 단점
  - 과적합(Overfitting)으로 알고리즘 성능 감소
  - 과적합 방지를 위해 트리의 크기를 제한하는 튜닝 필요

![성능과 모델 복잡도 관계](%EC%9D%B4%EB%AF%B8%EC%A7%80%ED%8F%B4%EB%8F%84/%EC%84%B1%EB%8A%A5%EA%B3%BC%EB%AA%A8%EB%8D%B8%EB%B3%B5%EC%9E%A1%EB%8F%84%EA%B4%80%EA%B3%84.png)
- 모델 복잡도: 모델이 Data 포인트를 얼마만큼 복잡하게 분류/회귀 하려고 하는지
  - 모델 복잡도가 높을 수록 Data를 하나하나 분류
  - 모델 복잡도가 낮을 수록 Data의 추세만 분류
  - **따라서 파라미터 구조를 통해서 과소/과대 분류를 피하는 일반화 지점을 찾아야함**

#### 결정트리 과적합
> 데이터의 분류 기준이 너무 과하면 과적합 상태

- 과적합 예시
  - 만약 xy그래프 영역 중 동남쪽 방향에 A데이터만 10개 존재하고, 북서쪽 방향에 A데이터 1개, B데이터가 9개 존재한다고 가정
  - 결정 트리의 규칙노드는 `모든 데이터 집합의 분류가 결정`되어야 함
  - 즉, 북서쪽의 A데이터 1개를 분류하기 위해서 필요 이상의 데이터 분할(규칙 노드)

1. max_depth
    - 결정 트리의 최대 깊이 설정
    - 최대 깊이 설정으로 과소/과대 적합 방지 가능(max_depth가 너무 낮거나 높으면 오히려 일반화가 어려울 수 있음)
2. min_samples_split
    - 분할할 수 있는 최소 샘플 개수 설정
    - min_samples_split = 20으로 설정 시, 샘플 개수가 20개 이하면 분할 X
3. min_samples_leaf
    - 분할하고 난 뒤의 샘플 최소 개수 설정
    - min_samples_leaf = 6으로 설정 시, 분할 이후 샘플 개수는 최소 6개
4. 특성 중요도
    - 균일한 샘플이 들어있는 노드를 만들기 위해 어떤 특성이 주요하게 사용되는지
    - 특성 중요도가 가장 높게 측정되는 수치 = 가장 중요한 특성으로 머신러닝이 인식